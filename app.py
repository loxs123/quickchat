import os
import time
import random
import json
import httpx  # æ›¿ä»£ requestsï¼Œç”¨äºå¼‚æ­¥è°ƒç”¨
import re
import copy

import asyncio
from playwright.async_api import async_playwright
from quart import Quart, render_template, request, jsonify, Response, make_response, send_file

# import crawler
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)) + "/crawler")
from crawler.crawler_factory import CrawlerFactory
import crawler.config as cfg

from utils import chat_model_api, stream_chat_model_api

app = Quart(__name__)

# é…ç½®ä¸Šä¼ è·¯å¾„
UPLOAD_FOLDER = 'uploads'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

crawler_cache = {}
playwright = None
en2zh = {'xhs': 'å°çº¢ä¹¦', 'bili': 'å“”å“©å“”å“©', 'dy': 'æŠ–éŸ³', 'tieba': 'è´´å§', 'wb': 'å¾®åš', 'zhihu': 'çŸ¥ä¹'}

@app.before_serving
async def startup():
    global crawler_cache, playwright
    print("ğŸŒŸ App starting...")
    playwright = await async_playwright().start()
    # åˆå§‹åŒ– crawler_cache
    crawler_cache['å°çº¢ä¹¦'] = CrawlerFactory.create_crawler('xhs', playwright)
    await crawler_cache['å°çº¢ä¹¦'].start()
    crawler_cache['å“”å“©å“”å“©'] = CrawlerFactory.create_crawler('bili', playwright)
    await crawler_cache['å“”å“©å“”å“©'].start()
    # crawler_cache['æŠ–éŸ³'] = CrawlerFactory.create_crawler('dy', playwright)
    # await crawler_cache['æŠ–éŸ³'].start()
    crawler_cache['è´´å§'] = CrawlerFactory.create_crawler('tieba', playwright)
    await crawler_cache['è´´å§'].start()
    crawler_cache['å¾®åš'] = CrawlerFactory.create_crawler('wb', playwright)
    await crawler_cache['å¾®åš'].start()
    crawler_cache['çŸ¥ä¹'] = CrawlerFactory.create_crawler('zhihu', playwright)
    await crawler_cache['çŸ¥ä¹'].start()

    # await asyncio.gather(
    #     crawler_cache['å°çº¢ä¹¦'].start(),
    #     crawler_cache['å“”å“©å“”å“©'].start(),
    #     crawler_cache['æŠ–éŸ³'].start(),
    #     crawler_cache['è´´å§'].start(),
    #     crawler_cache['å¾®åš'].start(),
    #     crawler_cache['çŸ¥ä¹'].start()
    # )
    
@app.after_serving
async def shutdown():
    playwright.stop()
    print("App is shutting down...")
    # é€€å‡ºæ—¶éå†å¹¶å…³é—­æ‰€æœ‰ crawler
    for crawler in crawler_cache.values():
        if hasattr(crawler, 'close') and callable(crawler.close):
            try:
                await crawler.close()
            except Exception as e:
                print(f"Error logging out crawler: {e}")
    print('[DONE]')

@app.route("/loxs")
async def index():
    resp = await make_response(await render_template('index.html'))
    if not request.cookies.get('session_id'):
        resp.set_cookie('session_id', f'{time.time()}-{random.randint(1, 100)}')
    return resp

@app.route("/process", methods=["POST"])
async def process():
    data = await request.get_json()
    input_text = data.get("text", "")
    mode = data.get("mode", "é»˜è®¤")

    output = await chat_model_api(input_text, mode)
    return jsonify({"result": output})

@app.route("/stream_process", methods=["POST"])
async def stream_process():
    data = await request.get_json()
    input_text = data.get("text", "")
    mode = data.get("mode", "é»˜è®¤")
    if mode != 'search':
        return Response(stream_chat_model_api(input_text, mode), content_type="text/event-stream")
    else:
        cfgs = re.findall(r'\[(.*?)\]', input_text)
        cfgs = {cfg.split('=')[0] : cfg.split('=')[1] for cfg in cfgs}
        keywords = re.sub(r'\[.*?\]', '', input_text)
        cfgs['KEYWORDS'] = keywords
        platforms_sort = ['è´´å§', 'å¾®åš', 'å“”å“©å“”å“©', 'å°çº¢ä¹¦', 'çŸ¥ä¹', 'æŠ–éŸ³']
        if 'P' in cfgs:
            platforms = cfgs['P'].split(',')
            platforms = [en2zh[p.strip()] for p in platforms if p.strip() in en2zh]
        else:
            platforms = list(crawler_cache.keys())
        platforms.sort(key = lambda x: platforms_sort.index(x))

        async def generate():
            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©ä¸åŒçš„å¹³å°
            abnormals = []
            for p in platforms:
                cnt = 0
                crawler = crawler_cache[p]
                async for note in crawler.search(**cfgs):
                    yield f"data: ## {p}\n{note}\n\n-$#$-"
                    cnt += 1
                if cnt == 0:
                    abnormals.append(p)

            summary = 'è¯·æ±‚çš„å¹³å°ä¸ºï¼š' + ', '.join(platforms)

            if len(abnormals) > 0:
                summary += ', ä½†ä»¥ä¸‹å¹³å°æ²¡æœ‰è·å–æ•°æ®ï¼š' + ', '.join(abnormals)
            else:
                summary += ', å‡è¯·æ±‚æˆåŠŸ'

            yield f"data: {summary}\n\n-$#$-"
        return Response(generate(), content_type="text/event-stream")


@app.route("/upload", methods=["POST"])
async def upload():
    if "files" not in (await request.files):
        return jsonify({"error": "No file part"}), 400
    session_id = request.cookies.get('session_id')
    files = (await request.files).getlist("files")
    saved_files = []
    for file in files:
        if file.filename == "":
            continue
        filename = f'{session_id}.' + secure_filename(file.filename)
        save_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        await file.save(save_path)
        saved_files.append(file.filename)

    return jsonify({"uploaded": saved_files})


if __name__ == "__main__":
    import asyncio
    app.run(host='127.0.0.1', port=5001, debug=True)

# if __name__ == "__main__":
#     import hypercorn.asyncio
#     from hypercorn.config import Config
#     config = Config()
#     config.bind = ["0.0.0.0:80"]
#     config.startup_timeout = 500  # å»¶é•¿ lifespan startup é˜¶æ®µçš„ç­‰å¾…æ—¶é—´
#     asyncio.run(hypercorn.asyncio.serve(app, config))
